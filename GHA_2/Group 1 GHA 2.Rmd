---
author: "Cyril Janak, Jonas Husmann, Niklas Kampe, Robin Scherrer"
date: "08.12.2021"
geometry: "left= 2cm, right = 2cm, top = 2.5cm, bottom = 2.5cm"
output: 
   pdf_document:
      includes:
         in_header: rmarkdown/style.sty
      number_sections: true
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
knitr::read_chunk('Group 1 GHA 2.R')
```

\thispagestyle{empty}
\input{rmarkdown/titlepage.tex}

\pagebreak

\thispagestyle{empty}
\tableofcontents

\pagebreak
\setcounter{page}{1}

# Requirements {-}
To solve the following tasks, the required libraries and the data sets are loaded 
first.
```{r requirements, echo=TRUE, warning=FALSE}
```

# Exercise 1 {-}
There are 214 observations in the training data set and 143 observations in the test 
data set.
```{r exercise_1, echo=TRUE, warning=FALSE}
```

# Exercise 2 {-}
The average grade is ~11.64, the minimum grade is 4 and the maximum grade is 19. All 
numbers were calculated using the training data.
```{r exercise_2, echo=TRUE, warning=FALSE}
```

# Exercise 3 {-}
```{r exercise_3, echo=TRUE, warning=FALSE}
```


# Exercise 4 {-}
<!-- When doing causal modeling there are independent variables (x_1,..,x_n) which are considered as the cause of the dependent variable (y), therefore one would expect a direct impact of the independent variables on the dependent variable. For predictive modelling the goal is to establish a method that allows to make predictions of the dependent variable (y) based on the known independent variables (x_1,..,x_n).  -->

Predictive modeling is used to predict an object of interest (e.g. forecasting or 
nowcasting) by using predictors (covariates). The goal is to get as good out-of-sample 
predictions as possible (e.g. predicting unemployment). The goal of causal modeling, 
in contrast, is to establish a causal relationship between the explanatory variables 
(covariates) and the object of interest (e.g. causal effect of inflation, GDP per 
capita, ... on unemployment). While for predictive modeling only the full rank 
condition is mandatory, for causal modeling both the full rank condition and the 
exclusion restriction must be fulfilled.

# Exercise 5 {-}

```{r exercise_5, echo=TRUE, warning=FALSE}
```
In order to elaborate the in-sample fit of the two models, we define the coefficient of determination R^2 as well as the in-sample MSE as the key fit determinants. From the results, we can observe that the first linear model with five covariates has a R^2 of 0.15 and an in-sample MSE of 8.99, whereas the second linear model including the first order interactions has an R^2 of 0.21 and an in-sample MSE of 8.38. From these results, we can conclude that the second model performns better in both fit coefficients, which is in accordance with the general result that an increased number of covariates often leads to better in-sample fits (or delivers the same model fit). Nevertheless, the both R^2s aand MSEs are relatively low/high, latter compared to the level of the dependent variable, which concludes an overall weak model fit.

\pagebreak

# Exercise 6 {-}
```{r exercise_6, echo=TRUE, warning=FALSE}
```

In order to determine the best-performing model, we define the out-of-sample fit plot as well as the out-of-sample MSE as the main determinants, based on the fact that the purpose of a prediction model is to perform best on the test sample. From the results, we can observe that the four OLS models, as described in the formulas above, have out-of-sample MSEs of 10.10, 10.47, 9.71 and 12.47, respectively. This result is further underlines in the fit plots in which the prediction of the third model shows the least deviation from the true values of the dependent variable in the test sample. Hence, we can conclude that the third model, namely the linear regression based on OLS with ten different covariates, performs best in the out-of-sample/test data. Thus, a better prediction performance on new data is expected compared to the three other models, even if the fourth model shows the best in-sample performance, which leads to the conclusion of overfitting in the third model.
